{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a10d2d53-9a8d-4839-89a0-c322cac108a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "checkpoint= \"abfss://checkpoint@adlsexamplebiju.dfs.core.windows.net\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0639c5f0-76d3-4981-bc58-be0be5366fa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def handle_null_values(df):\n",
    "    print(\"****Handling NULL Values****\")\n",
    "    print(\"Replacing NULL values on String Columns with 'Unknown'\")\n",
    "    df = df.fillna('Unknown')\n",
    "    print('Success!! ')\n",
    "\n",
    "    print(\"Replacing NULL values on Numeric Columns with 0\")\n",
    "    df = df.fillna(0)\n",
    "    print('Success!! ')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72f669d0-3276-4f28-8bd1-d452c01f79e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def remove_duplicates(df):\n",
    "    print(\"****Removing Duplicates****\")\n",
    "    df = df.dropDuplicates()\n",
    "    print('Duplicates Removed!! ')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f907357-581b-4c75-b442-1250a877d09b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_BronzeTrafficTable():\n",
    "    print(\"Reading the Bronze Table Data\")\n",
    "    df_bronzeTraffic = (spark.readStream\n",
    "                    .table(\"my_catalog.bronze.raw_traffic\")\n",
    "                    )\n",
    "    print(f'Reading MY_catalog.bronze.raw_traffic Success!')\n",
    "    return df_bronzeTraffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e9a6473-0c1c-42d5-b63c-2180bb965f6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4c24e8d-17e1-44dd-bf62-99fbba740501",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "read_BronzeTrafficTable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71be4b63-a1db-4cc4-b242-e05b2af0c65f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7c93787-1d11-4ae7-8516-aba0e880f3f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_traffics_data(df):\n",
    "    print(\"****Processing Roads Data****\")\n",
    "\n",
    "    # Create Electric Vehicles Count Column\n",
    "    print('Creating Electric Vehicles Count Column')\n",
    "    df = df.withColumn('Electric_Vehicles_Count',\n",
    "                        F.col('EV_Car') + F.col('EV_Bike'))\n",
    "    print('Success!! ')\n",
    "\n",
    "    \n",
    "    \n",
    "    # Create All Motor Vehicles Count Column\n",
    "    print('Creating All Motor Vehicles Count Column')\n",
    "    df = df.withColumn('Motor_Vehicles_Count',\n",
    "                        F.col('Electric_Vehicles_Count') +\n",
    "                        F.col('Two_wheeled_motor_vehicles') +\n",
    "                        F.col('Cars_and_taxis') +\n",
    "                        F.col('Buses_and_coaches') +\n",
    "                        F.col('LGV_Type') +\n",
    "                        F.col('HGV_Type'))\n",
    "    print('Success!! ')\n",
    "\n",
    "    # Add a Quality Check Status Column\n",
    "    df = df.withColumn(\"Quality_Check\", F.lit(\"Passed\"))\n",
    "    \n",
    "    # Add a Transformed Time Column\n",
    "    df = df.withColumn(\"Transformed_Time\", current_timestamp())\n",
    "\n",
    "    # Fix \"Count_date\" Date Format\n",
    "    df = df.withColumn(\n",
    "        \"Count_date\",\n",
    "        F.to_date(F.to_timestamp(F.col(\"Count_date\"), \"M/d/yyyy H:mm\"))  # Flexible format for 1 or 2 digits\n",
    "    )\n",
    "\n",
    "    print(\"****Processing Completed****\")\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcb9d7c1-c877-4c80-98f5-6b76f01218dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = read_BronzeTrafficTable()\n",
    "process_traffics_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a3103c8-cc36-44db-abf4-3bb7b62e02f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_BronzeRoadsTable():\n",
    "    print(\"Reading the Bronze Roads Table Data\")\n",
    "    df_bronzeRoads = (spark.readStream\n",
    "                    .table(\"my_catalog.bronze.raw_roads\")\n",
    "                    )\n",
    "    print(f'Reading my_catalog.bronze.raw_roads Success!')\n",
    "    return df_bronzeRoads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa0d2378-378f-4e8b-ad38-35661366fc8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def road_Category(df):\n",
    "    print('Creating Road Category Name Column: ', end='')\n",
    "    from pyspark.sql.functions import when,col\n",
    "\n",
    "    df_road_Cat = df.withColumn(\"Road_Category_Name\",\n",
    "                  when(col('Road_Category_ID') == '3', 'Class A Trunk Road')\n",
    "                  .when(col('Road_Category_ID') == '1', 'Class A Trunk Motor')\n",
    "                   .when(col('Road_Category_ID') == '4','Class A Principal road')\n",
    "                    .when(col('Road_Category_ID') == '2','Class A Principal Motorway')\n",
    "                    .when(col('Road_Category_ID') == '5','Class B road')\n",
    "                    .otherwise('NA')\n",
    "                  \n",
    "                  )\n",
    "    print('Success!! ')\n",
    "    print('***********************')\n",
    "    return df_road_Cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa3f1942-43a5-4783-9207-a75560050de5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def road_Type(df):\n",
    "    print('Creating Road Type Name Column: ', end='')\n",
    "    from pyspark.sql.functions import when,col\n",
    "\n",
    "    df_road_Type = df.withColumn(\"Road_Type\",\n",
    "                  when(col('Road_Category_Name').like('%Class A%'),'Major')\n",
    "                  .when(col('Road_Category_Name').like('%Class B%'),'Minor')\n",
    "                    .otherwise('NA')\n",
    "                  \n",
    "                  )\n",
    "    print('Success!! ')\n",
    "    print('***********************')\n",
    "    return df_road_Type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "820c6cbb-02b0-4484-812e-b81b32962553",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_Roads_SilverTable(StreamingDF):\n",
    "    print('Writing the silver_roads Data : ',end='') \n",
    "\n",
    "    write_StreamSilver_R = (StreamingDF.writeStream\n",
    "                .format('delta')\n",
    "                .option('checkpointLocation',checkpoint+ \"/SilverRoadsLoad/Checkpt/\")\n",
    "                .outputMode('append')\n",
    "                .queryName(\"SilverRoadsWriteStream\")\n",
    "                .trigger(availableNow=True)\n",
    "                .toTable(\"my_catalog.silver.silver_roads\"))\n",
    "    \n",
    "    write_StreamSilver_R.awaitTermination()\n",
    "    print(f'Writing my_catalog.silver.silver_roads Success!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29220700-117c-40a0-a986-7a12840d5d8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_Traffic_SilverTable(StreamingDF):\n",
    "    print(\"Writing the silver_traffic Data\") \n",
    "\n",
    "    write_StreamSilver = (StreamingDF.writeStream\n",
    "                .format('delta')\n",
    "                .option('checkpointLocation',checkpoint+ \"/SilverTrafficLoad/Checkpt/\")\n",
    "                .outputMode('append')\n",
    "                .queryName(\"SilverTrafficWriteStream\")\n",
    "                .trigger(availableNow=True)\n",
    "                .toTable(\"my_catalog.silver.silver_traffic\"))\n",
    "    \n",
    "    write_StreamSilver.awaitTermination()\n",
    "    print(f'Writing silver_traffic Success!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14b0f0d8-caf7-4eb2-b177-481e3e04dfb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bronzeTraffic = read_BronzeTrafficTable()\n",
    "df_dup = remove_duplicates(df_bronzeTraffic)\n",
    "df_null = handle_null_values(df_dup)\n",
    "df_process=process_traffics_data(df_null)\n",
    "\n",
    "df_bronzeRoads = read_BronzeRoadsTable()\n",
    "df_Roadsdup = remove_duplicates(df_bronzeRoads)\n",
    "df_Roadsnull = handle_null_values(df_Roadsdup)\n",
    "df_roadCat = road_Category(df_Roadsnull)\n",
    "df_type = road_Type(df_roadCat)\n",
    "write_Roads_SilverTable(df_type)\n",
    "write_Traffic_SilverTable(df_process)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6950069274233146,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03_load_to_silver.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
